{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "This notebook implements cross validation, a technique used in machine learning to evaluate the performance of a model on a limited dataset. The goal of cross validation is to estimate how well the model is likely to perform on new, unseen data. The basic idea is to split the available data into several parts, or folds, and train and test the model on different combinations of these folds. By averaging the performance across the different folds, we can get a more reliable estimate of the model's performance than if we had only trained and tested it on a single split of the data.\n",
    "\n",
    "The most common form of cross validation is k-fold cross validation, where the data is divided into k roughly equal parts. The model is then trained on k-1 of these folds and tested on the remaining fold. This process is repeated k times, with each of the k folds used once as the test set. The results of the k tests are averaged to produce an overall estimate of the model's performance. Cross validation can help to reduce the risk of overfitting, where the model fits too closely to the training data and performs poorly on new, unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "First, load the relevant libraries needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Import functions for modeling and evaluating performance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The model will be trained using the [Hawks](https://github.com/kary5678/INDE-577/blob/main/Data/hawks.csv) dataset. This dataset contains observations for three species of hawks, and attributes such as age, sex, wing length, body weight, tail length, etc. \n",
    "\n",
    "The code block below reads the dataset into a pandas DataFrame object, subsets the DataFrame to the relevant variables, and drops any rows where there are missing values for these relevant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>Wing</th>\n",
       "      <th>Tail</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Culmen</th>\n",
       "      <th>Hallux</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT</td>\n",
       "      <td>385.0</td>\n",
       "      <td>219</td>\n",
       "      <td>920.0</td>\n",
       "      <td>25.7</td>\n",
       "      <td>30.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT</td>\n",
       "      <td>381.0</td>\n",
       "      <td>235</td>\n",
       "      <td>990.0</td>\n",
       "      <td>26.7</td>\n",
       "      <td>31.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CH</td>\n",
       "      <td>265.0</td>\n",
       "      <td>220</td>\n",
       "      <td>470.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>23.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SS</td>\n",
       "      <td>205.0</td>\n",
       "      <td>157</td>\n",
       "      <td>170.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>14.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RT</td>\n",
       "      <td>412.0</td>\n",
       "      <td>230</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>28.5</td>\n",
       "      <td>32.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903</th>\n",
       "      <td>RT</td>\n",
       "      <td>380.0</td>\n",
       "      <td>224</td>\n",
       "      <td>1525.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>27.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>904</th>\n",
       "      <td>SS</td>\n",
       "      <td>190.0</td>\n",
       "      <td>150</td>\n",
       "      <td>175.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>RT</td>\n",
       "      <td>360.0</td>\n",
       "      <td>211</td>\n",
       "      <td>790.0</td>\n",
       "      <td>21.9</td>\n",
       "      <td>27.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>RT</td>\n",
       "      <td>369.0</td>\n",
       "      <td>207</td>\n",
       "      <td>860.0</td>\n",
       "      <td>25.2</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>RT</td>\n",
       "      <td>199.0</td>\n",
       "      <td>222</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>28.7</td>\n",
       "      <td>32.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Species   Wing  Tail  Weight  Culmen  Hallux\n",
       "0        RT  385.0   219   920.0    25.7    30.1\n",
       "2        RT  381.0   235   990.0    26.7    31.3\n",
       "3        CH  265.0   220   470.0    18.7    23.5\n",
       "4        SS  205.0   157   170.0    12.5    14.3\n",
       "5        RT  412.0   230  1090.0    28.5    32.2\n",
       "..      ...    ...   ...     ...     ...     ...\n",
       "903      RT  380.0   224  1525.0    26.0    27.6\n",
       "904      SS  190.0   150   175.0    12.7    15.4\n",
       "905      RT  360.0   211   790.0    21.9    27.6\n",
       "906      RT  369.0   207   860.0    25.2    28.0\n",
       "907      RT  199.0   222  1290.0    28.7    32.1\n",
       "\n",
       "[891 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data and subset it to the relevant columns/observations\n",
    "hawks = pd.read_csv(\"../../Data/hawks.csv\")\n",
    "hawks = hawks[[\"Species\", \"Wing\", \"Tail\", \"Weight\", \"Culmen\", \"Hallux\"]].dropna(axis=0)\n",
    "hawks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Performance of decision tree\n",
    "\n",
    "To have a point of reference in discussion of cross validation results, first I will implement a decision tree as I did in my [decision trees notebook](https://github.com/kary5678/INDE-577/blob/main/supervised-learning/decision_trees/decision_trees.ipynb), using wing and tail length as predictors for species and the hyperparameter `max_depth=3`.\n",
    "\n",
    "The processed hawks data is randomly split into a training and testing set using the traditional 80-20 rule of the Pareto Principle. The parameter `random_state=1` is used to ensure that we get the same observations in the training/testing set as in the Hawks exploratory analysis notebook [here](https://github.com/kary5678/INDE-577/blob/main/Data/hawks_analysis.ipynb). We know from the plots in `hawks_analysis.ipynb` that the split using this `random_state` produces a training set that is a good representation for the data being tested (and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation step\n",
    "X = hawks[[\"Wing\", \"Tail\"]]\n",
    "y = hawks[\"Species\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.986\n",
      "Testing accuracy: 0.972\n"
     ]
    }
   ],
   "source": [
    "decision_tree = DecisionTreeClassifier(max_depth = 3, random_state = 42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Training accuracy: {decision_tree.score(X_train, y_train):.3f}\")\n",
    "print(f\"Testing accuracy: {decision_tree.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing accuracy is what will be used as a frame of reference for cross validation to determine whether this model performs well or not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform k-fold cross validation\n",
    "\n",
    "I will perform k-fold cross validation with $k=10$, meaning the dataset will be split into 10 equal parts, and the model will be trained and evaluated 10 times. The `shuffle = True` parameter will randomly shuffle the data before splitting it into folds, ensuring that the folds are not biased towards any particular subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96666667 0.98876404 0.97752809 0.97752809 0.98876404 0.93258427\n",
      " 0.97752809 0.97752809 0.98876404 0.96629213]\n"
     ]
    }
   ],
   "source": [
    "# Define number of folds for cross validation\n",
    "n_folds = 10\n",
    "\n",
    "# Perform cross validation\n",
    "kf = KFold(n_splits = n_folds, shuffle = True, random_state = 42)\n",
    "scores = cross_val_score(decision_tree, X, y, cv = kf)\n",
    "print(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above array is the accuracy of the model over the 10 folds.\n",
    "\n",
    "To consolidate this into one metric, output the average accuracy of the model over the 10 folds, along with the 95% confidence interval (2 $\\times$ the standard deviation of the accuracy scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.974 (+/- 0.032)\n"
     ]
    }
   ],
   "source": [
    "# Print the average score and standard deviation\n",
    "print(\"Accuracy: {:.3f} (+/- {:.3f})\".format(scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the score obtained from k-fold cross validation is the same as the score obtained from the fitted model, then the model is not overfitting or underfitting the data; it is generalizing well to new data. On the other hand, if the score obtained from k-fold cross validation is slightly higher than the testing accuracy, it may indicate that that model is slightly overfit to the training data.\n",
    "\n",
    "For this decision tree model, the average score of 0.974 is slightly higher than the testing accuracy of 0.972, signaling the decision tree might be overfit to the data. However, considering that 0.972 is within the 0.032 margin, cross validation tells us that the fitted decision tree generalizes well!\n",
    "\n",
    "In conclusion, cross validation is a useful technique to evaluate the generalized performance of a model by allowing you to assess how well the model performs on new, unseen data, more than the traditional single training and testing set methodology."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
