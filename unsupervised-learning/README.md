# Unsupervised Learning

In unsupervised learning, we do not have any labels for the data unlike in supervised learning. The goal of unsupervised learning is to find patterns or structure in a dataset without any labeled output, finding meaningful relationships and groupings by exploring the data on its own. 

Two common supervised unsupervised learning tasks are **clustering** and **dimensionality reduction**. Clustering algorithms group data points into clusters based on their similarities, while dimensionality reduction techniques reduce the number of features in a dataset while preserving the most important information.

Unsupervised learning has a wide range of applications in various fields, such as:

* Customer segmentation: Clustering techniques can be used to group customers with similar characteristics, enabling businesses to tailor their marketing strategies to different customer groups.
* Anomaly detection: Clustering can help identify unusual or anomalous behavior in data, which can be useful in fraud detection, network intrusion detection, and other security applications.
* Image and speech recognition: Dimensionality reduction techniques can be used to reduce the complexity of image and speech data, making it easier to analyze and classify.
* Neuroscience: Clustering algorithms can help identify different types of neurons and map neural pathways in the brain, while dimensionality reduction techniques can help reduce the complexity of brain data for easier analysis.

### Heuristics

Another key difference of unsupervised learning from supervised learning is that without labeled outputs, there is no way to measure success in terms of something concrete like classification error or mean squared error. Instead, heuristics, or general guidelines or rules of thumb that can be used to guide the development or application of a particular algorithm or technique without necessarily providing a formal evaluation metric, are used in unsupervised learning tasks. For example:

* Start with simple algorithms: Simple algorithms such as k-means clustering or principal component analysis (PCA) are often a good starting point for exploring data and identifying patterns. More complex algorithms can be applied once the simpler ones have been tried and tested.
* Choose the right number of clusters: In clustering tasks, it's important to choose the right number of clusters to avoid overfitting or underfitting the data. One heuristic is to use the elbow method, where you plot the within-cluster sum of squares against the number of clusters and choose the number of clusters where the curve starts to level off.
* Normalize the data: Normalizing the data can help ensure that all features are given equal weight in clustering and dimensionality reduction algorithms. This can improve the accuracy and stability of the algorithms.
* Visualize the data: Visualizing the data can help identify patterns and outliers, and can also help in choosing the right algorithm and parameters.
* Use ensemble methods where multiple unsupervised learning algorithms are combined to obtain better results than using a single algorithm. This can help overcome the limitations of individual algorithms and improve the robustness of the results.

There are still various metrics that are commonly used to evaluate the performance of unsupervised learning algorithms. For example, the following metrics are used to evaluate performance in clustering tasks:

* Silhouette score: measures how well each data point fits into its assigned cluster compared to other clusters
* Within-cluster sum of squares: measures the compactness of each cluster in relation to the other clusters
* Purity: measures the proportion of data points in a cluster that belong to the same class or group
* Adjusted Rand Index: measures the similarity between the true labels of the data and the predicted labels generated by the clustering algorithm

## Directory Contents

To learn more about each algorithm and see them in application, visit their respective linked subdirectories.

### Clustering
* [*k*-means Clustering](https://github.com/kary5678/INDE-577/tree/main/unsupervised-learning/k-means_clustering)
* [Density-Based Spatial Clustering of Applications With Noise](https://github.com/kary5678/INDE-577/tree/main/unsupervised-learning/dbscan)


### Dimensionality Reduction
* [Principal Component Analysis](https://github.com/kary5678/INDE-577/tree/main/unsupervised-learning/pca)
